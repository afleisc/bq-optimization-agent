sources:
  my-bigquery-source:
    kind: bigquery
    project: dannydeleo
    location: us
tools:
  last-failed-jobs:
    kind: bigquery-sql
    source: my-bigquery-source
    description: Get the latest failed jobs.
    statement: >-
      SELECT job_id, creation_time, user_email, error_result 
      FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT 
      WHERE error_result.reason != "Null" 
      ORDER BY creation_time DESC LIMIT 3;
  top-jobs-extract:
    kind: bigquery-sql
    source: my-bigquery-source
    description: Get the latest failed jobs.
    statement: >-
      CREATE SCHEMA IF NOT EXISTS optimization_workshop;
      CREATE OR REPLACE TABLE optimization_workshop.top_jobs AS 
      SELECT
      job_id,
      query,
      user_email,
      total_slot_ms
      FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
      WHERE EXTRACT(DATE FROM  creation_time) = current_date()
      ORDER BY total_slot_ms DESC
      LIMIT 3;
  daily-project-analysis-extract:
    kind: bigquery-sql
    source: my-bigquery-source
    description: creates a table called daily project analysis in the optimization_workshop dataset. It contains daily slot consumption information about BigQuery jobs.
    statement: >-
      CREATE SCHEMA IF NOT EXISTS optimization_workshop;
      CREATE OR REPLACE TABLE optimization_workshop.daily_project_analysis AS 
      SELECT
      day, 
      project_id,
      COUNT(*)                            AS job_count,
      AVG(total_slot_ms)/1000             AS avg_total_slot_secs,
      MAX(median_total_slot_ms)/1000      AS median_total_slot_secs,
      MAX(p80_total_slot_ms)/1000         AS p80_total_slot_secs,
      SUM(total_slot_ms)/1000/60/60       AS total_slot_hours,
      AVG(time_secs)                      AS avg_time_secs,
      MAX(median_time_secs)               AS median_time_secs,
      SUM(time_secs)/60/60                AS total_time_hours,
      MAX(p80_time_secs)                  AS p80_time_secs,
      AVG(bytes_scanned)/POW(1024,3)      AS avg_gb_scanned,
      MAX(p80_bytes_scanned)/POW(1024,3)  AS p80_gb_scanned,
      SUM(bytes_scanned)/POW(1024,4)      AS total_tb_scanned,
      AVG(bytes_shuffled)/POW(1024,3)     AS avg_gb_shuffled,
      MAX(p80_bytes_shuffled)/POW(1024,3) AS p80_gb_shuffled,
      SUM(bytes_shuffled)/POW(1024,4)     AS total_tb_shuffled,
      AVG(bytes_spilled)/POW(1024,3)      AS avg_gb_spilled,
      MAX(p80_bytes_spilled)/POW(1024,3)  AS p80_gb_spilled,
      SUM(bytes_spilled)/POW(1024,4)      AS total_tb_spilled,
      FROM(
        SELECT
          day,
          project_id,
          total_slot_ms,
          PERCENTILE_CONT(total_slot_ms, 0.5) 
            OVER (PARTITION BY day, project_id)     AS median_total_slot_ms,
          PERCENTILE_CONT(total_slot_ms, 0.8) 
            OVER (PARTITION BY day, project_id)     AS p80_total_slot_ms,
          time_secs,
          PERCENTILE_CONT(time_secs, 0.5) 
            OVER (PARTITION BY day, project_id)     AS median_time_secs,
          PERCENTILE_CONT(time_secs, 0.8) 
            OVER (PARTITION BY day, project_id)     AS p80_time_secs,
          total_bytes_scanned bytes_scanned,
          PERCENTILE_CONT(total_bytes_scanned, 0.8) 
            OVER (PARTITION BY day, project_id)     AS p80_bytes_scanned,
          bytes_shuffled,
          PERCENTILE_CONT(bytes_shuffled, 0.8) 
            OVER (PARTITION BY day, project_id)     AS p80_bytes_shuffled,
          bytes_spilled,
          PERCENTILE_CONT(bytes_spilled, 0.8) 
            OVER (PARTITION BY day, project_id)     AS p80_bytes_spilled
        FROM(
          SELECT 
            DATE(jbo.creation_time) AS day,
            project_id,
            job_id,
            total_slot_ms,
            TIMESTAMP_DIFF(jbo.end_time,jbo.start_time, SECOND) AS time_secs,
            total_bytes_billed total_bytes_scanned,
            (SELECT SUM(stage.shuffle_output_bytes) FROM UNNEST(job_stages) stage) AS bytes_shuffled,
            (SELECT SUM(stage.shuffle_output_bytes_spilled) FROM UNNEST(job_stages) stage) AS bytes_spilled
          FROM
            `region-us`.INFORMATION_SCHEMA.JOBS_BY_ORGANIZATION jbo
          WHERE
            DATE(jbo.creation_time) >= CURRENT_DATE - 30
            -- Uncomment below to specify a list of projects to analyze
            -- AND jbo.project_id IN (<LIST_OF_PROJECT_IDS>)
            AND jbo.job_type = 'QUERY'
            AND jbo.end_time > jbo.start_time
            AND jbo.error_result IS NULL
            AND jbo.statement_type != 'SCRIPT'
      ))
      GROUP BY 1, 2;
  daily-project-analysis-assessment:
    kind: bigquery-sql
    source: my-bigquery-source
    description: Project level analysis enables us to understand key metrics such as slot_time, bytes_scanned, bytes_shuffled and bytes_spilled on a daily basis within a project. The metrics are examined as averages, medians and p80s. This enables us to understand at a high level what jobs within a project consume 80% of the time and 50% of the time daily.
    statement: >- 
      SELECT *
      FROM optimization_workshop.daily_project_analysis
      ORDER BY total_slot_hours DESC
      LIMIT 100;
  top-jobs-assessment:
    kind: bigquery-sql
    source: my-bigquery-source
    description: Get the latest failed jobs.
    statement: >-
      SELECT *
      FROM optimization_workshop.top_jobs;
toolsets:
  bq-extraction-toolset:
    - top-jobs-extract
    - last-failed-jobs
    - daily-project-analysis-extract
  bq-assessment-toolset:
    - daily-project-analysis-assessment
    - top-jobs-assessment
